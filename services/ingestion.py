import re
import tempfile
from io import BytesIO
import datetime as _dt
import numpy as np

import pandas as pd
import streamlit as st
from services.db import get_db, init_database


# === Google Drive ç›¸å…³ ===
from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive
import os
import time
from pathlib import Path
from contextlib import contextmanager


import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl")
warnings.filterwarnings("ignore", category=pd.errors.DtypeWarning)

from services.db import get_db_path
from services.logger import init_logging, log_info, log_warning, log_error
init_logging()

import json

def sync_drive_with_database(drive_files):
    conn = get_db()

    # Drive å½“å‰æ–‡ä»¶é›†åˆ
    drive_set = set(drive_files)

    # æ•°æ®åº“è®°å½•çš„æ–‡ä»¶é›†åˆ
    db_files = pd.read_sql("SELECT source_file FROM ingestion_log", conn)
    db_set = set(db_files["source_file"].tolist())

    # è¢«åˆ é™¤çš„æ–‡ä»¶
    deleted_files = db_set - drive_set

    for file in deleted_files:
        # åˆ é™¤ transactions
        conn.execute("DELETE FROM transactions WHERE source_file = ?", (file,))

        # åˆ é™¤ inventory
        conn.execute("DELETE FROM inventory WHERE source_file = ?", (file,))

        # åˆ é™¤ log è®°å½•
        conn.execute("DELETE FROM ingestion_log WHERE source_file = ?", (file,))

    conn.commit()
    conn.close()

def _expected_count_json_path(main_db: str) -> Path:
    """
    æŠŠ expected_drive_file_count å­˜åœ¨ä¸»åº“æ—è¾¹çš„ json æ–‡ä»¶é‡Œ
    ä¾‹å¦‚ï¼š/path/main.db  ->  /path/main.ingest_meta.json
    """
    p = Path(main_db)
    return p.with_suffix(".ingest_meta.json")


def load_expected_drive_file_count(main_db: str):
    """
    è¯»å– json é‡Œçš„ expected_drive_file_count
    è¿”å› int æˆ– None
    """
    try:
        meta_path = _expected_count_json_path(main_db)
        if not meta_path.exists():
            return None
        data = json.loads(meta_path.read_text(encoding="utf-8"))
        val = data.get("expected_drive_file_count")
        return int(val) if val is not None else None
    except Exception as e:
        log_warning(f"âš ï¸ Failed to read expected_drive_file_count json: {e}")
        return None


def save_expected_drive_file_count(main_db: str, count: int):
    """
    åŸå­å†™å…¥ jsonï¼šå…ˆå†™ .tmp å† replaceï¼Œé¿å…å†™åˆ°ä¸€åŠæ–‡ä»¶åæ‰
    """
    try:
        meta_path = _expected_count_json_path(main_db)
        tmp_path = meta_path.with_suffix(meta_path.suffix + ".tmp")

        payload = {
            "expected_drive_file_count": int(count),
            "updated_at": time.time(),
        }

        tmp_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")
        os.replace(str(tmp_path), str(meta_path))
        return True
    except Exception as e:
        log_warning(f"âš ï¸ Failed to save expected_drive_file_count json: {e}")
        try:
            if 'tmp_path' in locals() and tmp_path.exists():
                tmp_path.unlink(missing_ok=True)
        except Exception:
            pass
        return False

def drive_get_content_file_with_retry(drive_file, local_path: str, retries: int = 3):
    # é€€é¿ï¼š1s / 3s / 7s
    waits = [1, 3, 7]
    last_err = None
    for i in range(retries):
        try:
            drive_file.GetContentFile(local_path)
            return True
        except Exception as e:
            last_err = e
            wait = waits[i] if i < len(waits) else waits[-1]
            log_warning(f"âš ï¸ Drive download failed (attempt {i+1}/{retries}) for {local_path}: {e}")
            time.sleep(wait)
    raise RuntimeError(f"Drive download failed after {retries} retries: {local_path}") from last_err


@contextmanager
def ingest_file_lock(stale_seconds: int = 60 * 60):
    """
    è·¨è¿›ç¨‹æ–‡ä»¶é”ï¼ˆdb/ingest.lockï¼‰
    - åŒä¸€æ—¶é—´åªå…è®¸ä¸€ä¸ª ingest
    - stale_seconds é˜²æ­¢å´©æºƒåæ­»é”
    """
    db_path = Path(get_db_path()).resolve()
    db_dir = db_path.parent                 # âœ… æ˜ç¡® db ç›®å½•
    lock_path = db_dir / "ingest.lock"      # âœ… é”æ–‡ä»¶å›ºå®šåœ¨ db/

    lock_fd = None

    def try_acquire():
        nonlocal lock_fd
        try:
            db_dir.mkdir(parents=True, exist_ok=True)
            lock_fd = os.open(
                str(lock_path),
                os.O_CREAT | os.O_EXCL | os.O_WRONLY
            )
            os.write(
                lock_fd,
                f"pid={os.getpid()} time={time.time()}\n".encode()
            )
            return True
        except FileExistsError:
            return False

    acquired = try_acquire()

    # --- å¤„ç†é™ˆæ—§é” ---
    if not acquired and lock_path.exists():
        try:
            age = time.time() - lock_path.stat().st_mtime
            if age > stale_seconds:
                log_warning(f"âš ï¸ Stale ingest.lock detected ({int(age)}s), removing")
                lock_path.unlink(missing_ok=True)
                acquired = try_acquire()
        except Exception:
            pass

    if not acquired:
        yield False
        return

    try:
        log_info(f"ğŸ”’ Ingest lock acquired: {lock_path}")
        yield True
    finally:
        try:
            if lock_fd is not None:
                os.close(lock_fd)
        except Exception:
            pass
        try:
            lock_path.unlink(missing_ok=True)
            log_info("ğŸ”“ Ingest lock released")
        except Exception:
            pass


FOLDER_ID = "1lPmmJdB75yhDx2j4FxCjBW5iLZH3RQSp"

# âœ… å…¨å±€ç¼“å­˜ drive å®ä¾‹
_drive_instance = None



def get_drive():
    """
    Fully robust Google Drive authentication:
    - Loads credentials.
    - If token expired â†’ try refresh.
    - If refresh fails (invalid_grant) â†’ delete token â†’ force re-auth.
    - Always saves new token.
    """
    global _drive_instance
    if _drive_instance is not None:
        return _drive_instance

    gauth = GoogleAuth()

    # token ä¿å­˜è·¯å¾„ï¼ˆpydrive2 é»˜è®¤æ¨è token.jsonï¼‰
    TOKEN_PATH = "token.json"

    # å¦‚æœ token.json ä¸å­˜åœ¨ â†’ å¼ºåˆ¶é¦–æ¬¡ç™»å½•
    if not os.path.exists(TOKEN_PATH):
        log_info("ğŸ” Please sign in to Google Drive.")
        gauth.LocalWebserverAuth()
        gauth.SaveCredentialsFile(TOKEN_PATH)
        _drive_instance = GoogleDrive(gauth)
        return _drive_instance

    # Step 1 â€” load existing credentials
    if os.path.exists(TOKEN_PATH):
        try:
            gauth.LoadCredentialsFile(TOKEN_PATH)
        except Exception:
            os.remove(TOKEN_PATH)
            gauth.credentials = None

    # Step 2 â€” If credentials exist, try refresh
    if gauth.credentials is not None:
        try:
            if gauth.access_token_expired:
                gauth.Refresh()
            else:
                gauth.Authorize()

        except Exception as e:
            # refresh failed â†’ invalid_grant â†’ must re-auth
            print("âš ï¸ Token refresh failed:", e)
            try:
                os.remove(TOKEN_PATH)
            except:
                pass
            gauth.LocalWebserverAuth()

    else:
        # No token at all â†’ first-time login
        gauth.LocalWebserverAuth()

    # Step 3 â€” Save new token
    gauth.SaveCredentialsFile(TOKEN_PATH)

    _drive_instance = GoogleDrive(gauth)

    # --- log which account is authorized ---
    try:
        email = None

        # 1) preferred: id_token dict
        id_token = getattr(gauth.credentials, "id_token", None)
        if isinstance(id_token, dict):
            email = id_token.get("email")

        # 2) fallback: token_info (sometimes available)
        if not email:
            token_info = getattr(gauth.credentials, "token_info", None)
            if isinstance(token_info, dict):
                email = token_info.get("email")

        if email:
            log_info(f"ğŸ” Google Drive authorized as: {email}")
        else:
            log_info("ğŸ” Google Drive authorized (email not available in token).")

    except Exception as e:
        log_warning(f"âš ï¸ Could not read authorized email: {e}")

    return _drive_instance


def upload_file_to_drive(local_path: str, remote_name: str):
    """Upload file to Google Drive with success message."""
    try:
        drive = get_drive()  # now fully robust
        f = drive.CreateFile({'title': remote_name, 'parents': [{'id': FOLDER_ID}]})
        f.SetContentFile(local_path)
        f.Upload()
        log_info(f"â˜ï¸ Uploaded to Google Drive: {remote_name}")
        return True

    except Exception as e:
        log_warning(f"âš ï¸ Upload to Drive failed: {e}")
        return False


def download_file_from_drive(file_id, local_path):
    drive = get_drive()
    f = drive.CreateFile({'id': file_id})
    f.GetContentFile(local_path)


# --------------- å·¥å…·å‡½æ•° ---------------

def _fix_header(df: pd.DataFrame) -> pd.DataFrame:
    """è‹¥ç¬¬ä¸€è¡Œæ˜¯ Unnamedï¼Œå¤šæ•°æ˜¯å¤šè¡Œè¡¨å¤´ï¼›æŠŠç¬¬äºŒè¡Œæä¸ºè¡¨å¤´ã€‚"""
    if len(df.columns) and all(str(c).startswith("Unnamed") for c in df.columns):
        df.columns = df.iloc[0]
        df = df.drop(index=0).reset_index(drop=True)
    return df


def _to_float(series: pd.Series) -> pd.Series:
    return (
        series.astype(str)
        .str.replace(r"[^0-9\.\-]", "", regex=True)
        .replace("", pd.NA)
        .astype(float)
    )


def _extract_date_from_filename(name: str):
    """ä»æ–‡ä»¶åä¸­æå– YYYY-MM-DD"""
    m = re.search(r"(\d{4}-\d{2}-\d{2})", name)
    if m:
        return m.group(1)
    return None

def _extract_date_range_from_filename(name: str):
    """
    æ”¯æŒ items-YYYY-MM-DD-YYYY-MM-DD.csv è¿™ç§ï¼Œè¿”å› (start_date, end_date) å­—ç¬¦ä¸²
    ç”¨äºç¨³å®šæ’åº & è¯Šæ–­â€œæœ€æ—©å¯¼å…¥åˆ°å“ªä¸€å¤©â€
    """
    m = re.findall(r"(\d{4}-\d{2}-\d{2})", name or "")
    if not m:
        return (None, None)
    if len(m) == 1:
        return (m[0], None)
    return (m[0], m[1])


def list_all_files_in_folder(drive, folder_id: str):
    """
    å¼ºåˆ¶åˆ†é¡µæ‹‰å…¨é‡æ–‡ä»¶ï¼Œé¿å…åªæ‹¿åˆ°â€œå‰ N ä¸ªâ€å¯¼è‡´å†å²æ–‡ä»¶æ°¸è¿œæ²¡å¯¼å…¥ã€‚
    """
    q = f"'{folder_id}' in parents and trashed=false"
    params = {"q": q, "maxResults": 1000}
    all_files = []

    file_list = drive.ListFile(params)
    while True:
        batch = file_list.GetList()
        all_files.extend(batch)

        token = getattr(file_list, "metadata", {}).get("nextPageToken")
        if not token:
            break
        params["pageToken"] = token
        file_list = drive.ListFile(params)

    return all_files


# --------------- é¢„å¤„ç†ï¼ˆä¸æ”¹åˆ—åï¼‰ ---------------

def preprocess_transactions(df: pd.DataFrame) -> pd.DataFrame:
    df = _fix_header(df)
    if "Date" in df.columns and "Time" in df.columns:
        df["Datetime"] = pd.to_datetime(
            df["Date"].astype(str) + " " + df["Time"].astype(str),
            errors="coerce"
        )
        drop_cols = [c for c in ["Date", "Time", "Time Zone"] if c in df.columns]
        df = df.drop(columns=drop_cols)
    elif "Datetime" in df.columns:
        df["Datetime"] = pd.to_datetime(df["Datetime"], errors="coerce")

    for col in ["Net Sales", "Gross Sales", "Qty", "Discounts"]:
        if col in df.columns:
            df[col] = _to_float(df[col])

    # === æ–°å¢ï¼šCard Brand ä¸ PAN Suffix å¤„ç†ï¼Œä¿è¯å†™å…¥æ•°æ®åº“ ===
    if "Card Brand" in df.columns:
        df["Card Brand"] = (
            df["Card Brand"]
            .astype(str)
            .str.strip()
            .str.title()  # æ ‡å‡†åŒ–ä¸ºé¦–å­—æ¯å¤§å†™
        )

    if "PAN Suffix" in df.columns:
        df["PAN Suffix"] = (
            df["PAN Suffix"]
            .astype(str)
            .str.replace(r"\.0$", "", regex=True)  # å»æ‰æµ®ç‚¹å½¢å¼çš„".0"
            .str.strip()
        )
    # === NEW: Clean item names / remove leading '*' ===
    for col in ["Item", "Item Name", "Price Point Name"]:
        if col in df.columns:
            df[col] = (
                df[col]
                .astype(str)
                .str.replace(r'^\*+', '', regex=True)  # remove one or more leading *
                .str.strip()
            )

    # === è‡ªåŠ¨åˆ†ç±»ï¼šæ‰€æœ‰å«â€œkombuchaâ€å…³é”®å­—çš„é¡¹ç›®å½’ç±»ä¸º Drinks ===

    if "Item" in df.columns and "Category" in df.columns:
        df["Item_lower"] = df["Item"].astype(str).str.lower()

        # åŒ…å«ä»»ä½• "kombucha" å­—æ ·çš„ item â†’ Drinks
        kombucha_mask = df["Item_lower"].str.contains("kombucha", na=False)

        df.loc[kombucha_mask, "Category"] = "Drinks"

        # åˆ é™¤ä¸´æ—¶åˆ—
        df = df.drop(columns=["Item_lower"])

    return df


def preprocess_inventory(df: pd.DataFrame, filename: str = None) -> pd.DataFrame:
    df = _fix_header(df)

    # inventoryè¡¨æ ¼ä»ç¬¬äºŒè¡Œå¼€å§‹æ˜¯header
    if len(df) > 0 and all(str(col).startswith("Unnamed") for col in df.columns):
        df.columns = df.iloc[0]
        df = df[1:].reset_index(drop=True)
        df = _fix_header(df)  # å†æ¬¡å¤„ç†å¯èƒ½çš„å¤šè¡Œè¡¨å¤´

    # === NEW: Clean leading '*' from Item/Variation columns ===
    for col in ["Item", "Item Name", "Variation Name", "SKU"]:
        if col in df.columns:
            df[col] = (
                df[col]
                .astype(str)
                .str.replace(r'^\*+', '', regex=True)  # å»æ‰å¼€å¤´çš„æ‰€æœ‰æ˜Ÿå·
                .str.strip()
            )

    required = [
        "Tax - GST (10%)", "Price", "Current Quantity Vie Market & Bar",
        "Default Unit Cost", "Categories"
    ]
    for col in required:
        if col not in df.columns:
            df[col] = None

    # è¿‡æ»¤æ‰Current Quantity Vie Market & Baræˆ–è€…Default Unit Costä¸ºç©ºçš„è¡Œ
    if "Current Quantity Vie Market & Bar" in df.columns and "Default Unit Cost" in df.columns:
        for col in ["Price", "Current Quantity Vie Market & Bar", "Default Unit Cost"]:
            if col not in df.columns:
                df[col] = None
            df[col] = (
                df[col].astype(str)
                .str.replace(r"[^0-9\.\-]", "", regex=True)
                .replace("", pd.NA)
            )
            df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0)

    if filename:
        df["source_date"] = _extract_date_from_filename(filename)

    return df


def _is_transaction_file(filename: str, df: pd.DataFrame) -> bool:
    """åŒé‡åˆ¤æ–­ï¼šæ–‡ä»¶å + åˆ—å æ‰è®¤ä¸ºæ˜¯äº¤æ˜“æ–‡ä»¶"""
    fname = (filename or "").lower()

    # æ–‡ä»¶åå…³é”®è¯
    name_ok = (
        "item" in fname or
        "transaction" in fname or
        "sales" in fname
    )

    # åˆ—åå…³é”®è¯
    cols = {str(c).strip().lower() for c in df.columns}
    cols_ok = ("net sales" in cols and "gross sales" in cols)

    return name_ok and cols_ok


def _is_inventory_file(filename: str, df: pd.DataFrame) -> bool:
    """åŒé‡åˆ¤æ–­ï¼šæ–‡ä»¶å + åˆ—å æ‰è®¤ä¸ºæ˜¯åº“å­˜æ–‡ä»¶"""
    fname = (filename or "").lower()

    # æ–‡ä»¶åå…³é”®è¯
    name_ok = (
        "catalogue" in fname or
        "inventory" in fname or
        "stock" in fname
    )

    # åˆ—åå…³é”®è¯ï¼šè‡³å°‘å‡ºç°ä»»æ„ä¸€ä¸ª
    cols = {str(c).strip().lower() for c in df.columns}
    cols_ok = (
        "sku" in cols or
        "categories" in cols or
        "stock on hand" in cols
    )

    return name_ok and cols_ok


def _is_member_file(filename: str, df) -> bool:
    """
    åˆ¤æ–­ä¸€ä¸ªæ–‡ä»¶æ˜¯å¦ä¸º member æ–‡ä»¶ï¼š
    - æ–‡ä»¶ååŒ…å« 'member'ï¼ˆä¸åˆ†å¤§å°å†™ï¼‰ï¼Œæˆ–è€…
    - åˆ—åä¸­åŒæ—¶åŒ…å« First Name / Surname / Birthday ï¼ˆä¸åˆ†å¤§å°å†™ï¼‰
    """
    fname = (filename or "").lower()
    has_member_in_name = "member" in fname

    # df ä¸æ˜¯ DataFrame çš„æ—¶å€™ï¼Œä¸è¦ç›´æ¥è®¿é—® .columnsï¼Œä»¥å…æŠ¥é”™
    if not hasattr(df, "columns"):
        # é€€ä¸€æ­¥ï¼šå¦‚æœæ–‡ä»¶åé‡Œå†™äº† memberï¼Œå°±å½“æˆ member æ–‡ä»¶ï¼Œå¦åˆ™ç›´æ¥ False
        return has_member_in_name

    cols_lower = {str(c).strip().lower() for c in df.columns}
    has_core_cols = {"first name", "surname", "birthday"}.issubset(cols_lower)

    return has_member_in_name or has_core_cols



def preprocess_members(df: pd.DataFrame) -> pd.DataFrame:
    df = _fix_header(df)

    # 1) æ ‡å‡†åŒ–åˆ—åï¼šå…¨éƒ¨è½¬å°å†™ååšæ˜ å°„
    rename_map = {}
    for c in df.columns:
        cl = str(c).strip().lower()

        if cl == "surname":
            rename_map[c] = "Last Name"
        elif cl == "last name":
            rename_map[c] = "Last Name"
        elif cl == "first name":
            rename_map[c] = "First Name"
        elif cl == "square customer id":
            rename_map[c] = "Square Customer ID"
        elif cl == "email address":
            rename_map[c] = "Email Address"
        elif cl == "phone number":
            rename_map[c] = "Phone Number"
        elif cl == "creation date":
            rename_map[c] = "Creation Date"
        elif cl == "customer note":
            rename_map[c] = "Customer Note"
        elif cl == "reference id":
            rename_map[c] = "Reference ID"

    df = df.rename(columns=rename_map)

    # 2) æ¸…ç† Phone Number å­—æ®µ - ç§»é™¤æ³¨é‡Šæ–‡æœ¬
    if "Phone Number" in df.columns:
        def clean_phone(phone):
            if pd.isna(phone):
                return phone
            phone_str = str(phone)
            # æŸ¥æ‰¾æ‰‹æœºå·æ¨¡å¼ï¼šä»¥+61æˆ–61å¼€å¤´
            import re
            match = re.search(r'(\+?61\d{8,9})', phone_str)
            if match:
                return match.group(1)
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›åŸå§‹å€¼
            return phone_str

        df["Phone Number"] = df["Phone Number"].apply(clean_phone)

    # 3) æ¸…ç† Square Customer ID å’Œ Reference ID
    for col in ["Square Customer ID", "Reference ID"]:
        if col in df.columns:
            df[col] = df[col].astype(str).str.strip()

    # 4) åªä¿ç•™å’Œ DB å¯¹é½çš„åˆ—
    allowed_cols = [
        "Square Customer ID",
        "First Name",
        "Last Name",
        "Email Address",
        "Phone Number",
        "Creation Date",
        "Customer Note",
        "Reference ID",
    ]
    existing = [c for c in df.columns if c in allowed_cols]
    df = df[existing]

    # 5) ç®€å•æ¸…æ´—ï¼šå»ç©ºæ ¼
    for col in ["Square Customer ID", "First Name", "Last Name",
                "Email Address", "Phone Number", "Reference ID"]:
        if col in df.columns:
            df[col] = df[col].astype(str).str.strip()

    return df


# --------------- è¡¨ç»“æ„å¯¹é½ & å»é‡ & å†™å…¥ ---------------

def _table_exists(conn, table: str) -> bool:
    try:
        cur = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name=?", (table,))
        return cur.fetchone() is not None
    except Exception:
        return False


def _existing_columns(conn, table: str) -> list:
    try:
        cur = conn.execute(f"PRAGMA table_info('{table}')")
        return [row[1] for row in cur.fetchall()]
    except Exception:
        return []


def _add_missing_columns(conn, table: str, missing_cols: list, prefer_real: set):
    cur = conn.cursor()
    for col in missing_cols:
        coltype = "REAL" if col in prefer_real else "TEXT"
        cur.execute(f'''ALTER TABLE "{table}" ADD COLUMN "{col}" {coltype}''')
    conn.commit()


def _ensure_table_schema(conn, table: str, df: pd.DataFrame, prefer_real: set):
    if not _table_exists(conn, table):
        # å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œåˆ›å»ºè¡¨
        df.head(0).to_sql(table, conn, if_exists="replace", index=False)
        return
    cols_now = set(_existing_columns(conn, table))
    incoming = list(df.columns)
    missing = [c for c in incoming if c not in cols_now]
    if missing:
        _add_missing_columns(conn, table, missing, prefer_real)


def _deduplicate(df: pd.DataFrame, key_col: str, conn, table: str) -> pd.DataFrame:
    if df is None or df.empty:
        return df

    if table == "inventory" and "source_date" in df.columns and "SKU" in df.columns:
        try:
            # åªä¸åŒä¸€å¤©çš„æ•°æ®å»é‡
            exist = pd.read_sql('SELECT source_date, SKU FROM "inventory"', conn)
            exist["source_date"] = pd.to_datetime(exist["source_date"], errors="coerce").dt.date.astype(str)
            exist["SKU"] = exist["SKU"].astype(str)

            df_local = df.copy()
            df_local["source_date"] = pd.to_datetime(df_local["source_date"], errors="coerce").dt.date.astype(str)
            df_local["SKU"] = df_local["SKU"].astype(str)

            # âœ… åªä¸ç›¸åŒæ—¥æœŸæ¯”å¯¹ï¼Œè€Œéæ‰€æœ‰æ—¥æœŸ
            existed_keys = set((exist["source_date"] + "||" + exist["SKU"]).unique())
            keys = df_local["source_date"] + "||" + df_local["SKU"]

            mask = ~keys.isin(existed_keys)
            return df_local[mask]
        except Exception:
            return df

    # å…¶å®ƒè¡¨/åœºæ™¯ï¼šä¿æŒåŸå•é”®å»é‡é€»è¾‘
    if key_col not in df.columns:
        return df
    try:
        exist = pd.read_sql(f'''SELECT "{key_col}" FROM "{table}"''', conn)
        existed_set = set(exist[key_col].dropna().astype(str).unique())
        mask = ~df[key_col].astype(str).isin(existed_set)
        return df[mask]
    except Exception:
        return df


def _sqlite_sanitize_df(df: pd.DataFrame) -> pd.DataFrame:
    """æŠŠ pandas / numpy ç±»å‹è½¬æ¢æˆ sqlite3 æ”¯æŒçš„å‚æ•°ç±»å‹ã€‚"""
    out = df.copy()

    # 1) datetime64 åˆ— -> å­—ç¬¦ä¸²
    for col in out.columns:
        if pd.api.types.is_datetime64_any_dtype(out[col]):
            out[col] = out[col].dt.strftime("%Y-%m-%d %H:%M:%S")

    # 2) object åˆ—é‡Œå¯èƒ½æ··è¿› pandas.Timestampï¼ˆå°¤å…¶ Datetime è¢«é¢„å¤„ç†æˆ object æ—¶ï¼‰
    def fix_cell(x):
        if x is None:
            return None

        # NaN / NaT
        try:
            if pd.isna(x):
                return None
        except Exception:
            pass

        # pandas.Timestamp
        if isinstance(x, pd.Timestamp):
            if pd.isna(x):
                return None
            return x.to_pydatetime().strftime("%Y-%m-%d %H:%M:%S")

        # numpy.datetime64
        if isinstance(x, np.datetime64):
            try:
                ts = pd.to_datetime(x, errors="coerce")
                if pd.isna(ts):
                    return None
                return ts.to_pydatetime().strftime("%Y-%m-%d %H:%M:%S")
            except Exception:
                return None

        # datetime.datetime
        if isinstance(x, _dt.datetime):
            return x.strftime("%Y-%m-%d %H:%M:%S")

        # datetime.dateï¼ˆæ³¨æ„ï¼šdate æ²¡æœ‰æ—¶é—´ï¼‰
        if isinstance(x, _dt.date):
            return _dt.datetime(x.year, x.month, x.day).strftime("%Y-%m-%d %H:%M:%S")

        return x

    out = out.applymap(fix_cell)

    return out

def _write_df(conn, df: pd.DataFrame, table: str, key_candidates: list, prefer_real: set, is_initial_build: bool = False):
    """
    ç»Ÿä¸€å†™å…¥æ•°æ®åº“ï¼š
    - inventoryï¼šåŒä¸€å¤©å…ˆåˆ å†å†™ï¼ˆä¿ç•™ä½ åŸé€»è¾‘ï¼‰
    - membersï¼šå¢åˆ æ”¹ï¼ˆä¿ç•™ä½ åŸé€»è¾‘ï¼‰
    - transactionsï¼šâœ… ç”¨è¡Œçº§å¤åˆ key å»é‡ï¼ˆæ ¹æ²»é‡å æ–‡ä»¶åå†å²ï¼‰
    è¿”å›ï¼šå®é™…å†™å…¥ï¼ˆå»é‡åï¼‰è¡Œæ•° inserted
    """
    if df is None or df.empty:
        return 0

    # --- é€šç”¨ï¼šç¡®ä¿è¡¨ç»“æ„ä¸€è‡´ ---
    _ensure_table_schema(conn, table, df, prefer_real)

    # ----------------------------------------------------------------------
    # inventoryï¼šåŒä¸€å¤©å…ˆåˆ é™¤å†å†™å…¥
    # ----------------------------------------------------------------------
    if table == "inventory" and "source_date" in df.columns:
        dates = df["source_date"].dropna().unique().tolist()
        if dates:
            for d in dates:
                conn.execute(f'DELETE FROM "{table}" WHERE source_date=?', (d,))
            conn.commit()

    # ----------------------------------------------------------------------
    # membersï¼šå¢åˆ æ”¹ï¼ˆä¿ç•™ä½ åŸé€»è¾‘ï¼Œæœ€å return insertedï¼‰
    # ----------------------------------------------------------------------
    if table == "members":
        df = _sqlite_sanitize_df(df)
        try:
            df_old = pd.read_sql('SELECT * FROM "members"', conn)
        except Exception:
            df_old = pd.DataFrame()

        key = None
        for k in ["Square Customer ID", "Reference ID"]:
            if k in df.columns:
                key = k
                break
        if key is None:
            return 0

        df[key] = df[key].astype(str)
        if not df_old.empty:
            df_old[key] = df_old[key].astype(str)

        old_keys = set(df_old[key]) if not df_old.empty else set()
        new_keys = set(df[key])

        keys_to_delete = old_keys - new_keys
        if keys_to_delete:
            placeholders = ",".join(["?"] * len(keys_to_delete))
            conn.execute(
                f'DELETE FROM "members" WHERE "{key}" IN ({placeholders})',
                tuple(keys_to_delete)
            )

        keys_to_insert = new_keys - old_keys
        df_insert = df[df[key].isin(keys_to_insert)]
        inserted = int(len(df_insert))
        if not df_insert.empty:
            # âœ… 1) å†™å…¥å‰å† sanitize ä¸€æ¬¡ï¼ˆåŒä¿é™©ï¼‰
            df_insert = _sqlite_sanitize_df(df_insert)
            df_insert = df_insert.where(pd.notnull(df_insert), None)
            df_insert.to_sql("members", conn, if_exists="append", index=False)

        # ğŸš€ ç¬¬ä¸€æ¬¡å»ºåº“ï¼šä¸è¦é€è¡Œ UPDATEï¼ˆææ…¢ï¼‰ï¼Œåªæ’å…¥/åˆ é™¤å³å¯
        if (not is_initial_build) and (not df_old.empty):
            df_merge = df.merge(df_old, on=key, how="inner", suffixes=("_new", "_old"))
            update_cols = [c for c in df.columns if c != key]
            for _, row in df_merge.iterrows():
                changed = any(str(row[f"{c}_new"]) != str(row[f"{c}_old"]) for c in update_cols)
                if changed:
                    set_clause = ", ".join([f'"{c}"=?' for c in update_cols])
                    params = [row[f"{c}_new"] for c in update_cols] + [row[key]]
                    params = _sqlite_sanitize_df(pd.DataFrame([params])).iloc[0].tolist()
                    params = [None if (isinstance(x, float) and pd.isna(x)) else x for x in params]

                    conn.execute(
                        f'UPDATE members SET {set_clause} WHERE "{key}"=?',
                        params
                    )

        conn.commit()
        return inserted

    if table == "transactions":
        # 1) ä¿è¯è¿™äº›åˆ—éƒ½å­˜åœ¨
        needed = [
            "Transaction ID", "Datetime", "Item", "Net Sales", "Gross Sales",
            "Discounts", "Qty", "Customer ID", "Modifiers Applied",
            "Tax", "Card Brand", "PAN Suffix"
        ]
        for c in needed:
            if c not in df.columns:
                df[c] = ""

        df_local = df.copy()

        # 2) å…ˆåšä¸€ä¸ªâ€œè¡Œå†…å®¹ baseâ€ï¼ˆå°½é‡åŒ…å«å®Œæ•´ä¿¡æ¯ï¼‰
        base_cols = [
            "Transaction ID", "Datetime", "Item", "Net Sales", "Gross Sales",
            "Discounts", "Qty", "Customer ID", "Modifiers Applied",
            "Tax", "Card Brand", "PAN Suffix"
        ]
        for c in ["Net Sales", "Gross Sales", "Discounts", "Qty"]:
            if c in df_local.columns:
                df_local[c] = pd.to_numeric(df_local[c], errors="coerce").fillna(0)
                # é‡‘é¢ç»Ÿä¸€ä¿ç•™2ä½ï¼ŒQty å¯ä»¥ä¿ç•™3ä½æˆ–ä¸ round
                if c != "Qty":
                    df_local[c] = df_local[c].round(2)

        df_local["__base"] = df_local[base_cols].astype(str).agg("||".join, axis=1)

        # 3) å¯¹äºå®Œå…¨ç›¸åŒçš„è¡Œå†…å®¹ï¼Œåœ¨åŒä¸€ Transaction å†…ç»™ä¸€ä¸ªç¨³å®šçš„åºå·
        #    è¿™æ ·åŒä¸€å°ç¥¨é‡Œé‡å¤çš„ä¸¤è¡Œä¸ä¼šäº’ç›¸å
        df_local = df_local.sort_values(
            ["Transaction ID", "Datetime", "Item", "Net Sales", "Qty", "Customer ID", "__base"],
            kind="mergesort"
        )
        df_local["__dup_idx"] = df_local.groupby(["Transaction ID", "__base"]).cumcount()

        # 4) æœ€ç»ˆ row_key = base + dup_idx
        df_local["__row_key"] = df_local["__base"] + "||" + df_local["__dup_idx"].astype(str)

        # åªä¿ç•™ row_keyï¼Œåˆ æ‰ä¸´æ—¶åˆ—
        df_local = df_local.drop(columns=["__base", "__dup_idx"])

        # âœ…ç¡®ä¿åˆ—å­˜åœ¨ï¼ˆæ­£å¼åº“ / tmpåº“éƒ½è¦æœ‰è¿™åˆ—ï¼‰
        try:
            conn.execute('ALTER TABLE "transactions" ADD COLUMN "__row_key" TEXT')
        except Exception:
            pass

        # âœ…å”¯ä¸€ç´¢å¼•ï¼šé˜²æ­¢é‡å¤å†™å…¥
        try:
            conn.execute('CREATE UNIQUE INDEX IF NOT EXISTS idx_tx_row_key ON transactions("__row_key")')
        except Exception:
            pass

        # ç”¨ DB å”¯ä¸€ç´¢å¼•å…œåº•ï¼Œä¸ä¾èµ– _deduplicate æˆåŠŸä¸å¦
        cols = list(df_local.columns)
        placeholders = ",".join(["?"] * len(cols))
        col_sql = ",".join([f'"{c}"' for c in cols])
        sql = f'INSERT OR IGNORE INTO "transactions" ({col_sql}) VALUES ({placeholders})'

        # âœ… å…³é”®ï¼šå†™å…¥å‰æŠŠ Timestamp / NaT / NaN æ¸…æ‰
        df_local = _sqlite_sanitize_df(df_local)
        df_local = df_local.where(pd.notnull(df_local), None)

        before = conn.total_changes
        conn.executemany(sql, df_local.itertuples(index=False, name=None))
        conn.commit()
        inserted = conn.total_changes - before
        return int(inserted)

    # ----------------------------------------------------------------------
    # å…¶å®ƒè¡¨ï¼šä¿ç•™åŸå•é”®å»é‡
    # ----------------------------------------------------------------------
    key_col = next((k for k in key_candidates if k in df.columns), None)
    if key_col:
        df = _deduplicate(df, key_col, conn, table)

    inserted = int(len(df))
    if inserted > 0:
        df.to_sql(table, conn, if_exists="append", index=False)

    return inserted



# --------------- ç´¢å¼• ---------------
def ensure_indexes():
    """Ensure DB schema + indexes exist.

    IMPORTANT:
    - Do NOT create empty placeholder tables with pandas (it may create tables with 0 columns).
    - Always call init_database() which creates the correct schema and indexes.
    """
    try:
        init_database()
    except Exception as e:
        # Do not crash ingestion for index creation issues; just log.
        try:
            log_error(f"âŒ init_database() failed in ensure_indexes(): {e}")
        except Exception:
            pass


def ingest_from_drive_all():
    is_initial_build = not Path(get_db_path()).exists()
    with ingest_file_lock() as locked:
        if not locked:
            log_warning("â³ ingest already running (file lock), skip ingest_from_drive_all()")
            return False
        _ingest_from_drive_all_impl(is_initial_build=is_initial_build)
        return True


# --------------- ä» Google Drive å¯¼å…¥ ---------------
def _ingest_from_drive_all_impl(is_initial_build: bool = False):
    """
    âœ… æœ€ç»ˆç¨³æ€ï¼šä¸´æ—¶åº“ + æˆåŠŸåæ›¿æ¢æ­£å¼åº“
    - å…ˆæŠŠæ‰€æœ‰æ•°æ®å¯¼å…¥åˆ° main_db.tmp
    - å¯¼å…¥â€œè¶³å¤Ÿå®Œæ•´â€æ‰ç”¨ tmp åŸå­æ›¿æ¢ main_db
    - ä»»æ„ä¸­é€”å¤±è´¥/åŠæ®‹å¯¼å…¥ï¼šä¸¢å¼ƒ tmpï¼Œä¸ç ´åç°æœ‰æ­£å¼åº“
    """
    import shutil
    import sqlite3

    # --- 0) æ‰¾åˆ°æ­£å¼åº“è·¯å¾„ï¼ˆä¸ä¾èµ– services/db.py å¢åŠ æ–°å‡½æ•°ï¼‰---
    main_conn = get_db()
    try:
        row = main_conn.execute("PRAGMA database_list").fetchone()
        # row é€šå¸¸æ˜¯ (seq, name, file)
        main_db = row[2] if row and len(row) >= 3 else None
    finally:
        try:
            main_conn.close()
        except Exception:
            pass

    if not main_db:
        log_error("âŒ Cannot resolve main DB path (PRAGMA database_list empty). Abort ingest.")
        return

    tmp_db = main_db + ".tmp"
    bak_db = main_db + ".bak"

    # --- 1) æ¸…ç†æ—§ tmp ---
    try:
        if os.path.exists(tmp_db):
            os.remove(tmp_db)
    except Exception:
        pass

    tmp_conn = sqlite3.connect(tmp_db)
    tmp_conn.row_factory = sqlite3.Row

    # ğŸš€ å¤§å¹…æé€Ÿï¼šå»ºåº“é˜¶æ®µç”¨æ›´å¿«çš„å†™å…¥å‚æ•°ï¼ˆtmpåº“å®‰å…¨ï¼‰
    tmp_conn.execute("PRAGMA synchronous = OFF;")
    tmp_conn.execute("PRAGMA journal_mode = MEMORY;")
    tmp_conn.execute("PRAGMA temp_store = MEMORY;")
    tmp_conn.execute("PRAGMA cache_size = -200000;")  # ~200MB cacheï¼ˆå¯æŒ‰éœ€è°ƒå°ï¼‰
    tmp_conn.execute("PRAGMA locking_mode = EXCLUSIVE;")

    cur = tmp_conn.cursor()

    # ç”¨ä½  services/db.py é‡Œç›¸åŒç»“æ„å»ºè¡¨ï¼ˆæœ€å°å¤åˆ¶ï¼Œä¿è¯ä¸€è‡´ï¼‰
    cur.executescript("""
        CREATE TABLE IF NOT EXISTS transactions (
            Datetime TEXT,
            Category TEXT,
            Item TEXT,
            Qty REAL,
            [Net Sales] REAL,
            [Gross Sales] REAL,
            Discounts REAL,
            [Customer ID] TEXT,
            [Transaction ID] TEXT,
            Tax TEXT,
            [Card Brand] TEXT,
            [PAN Suffix] TEXT,
            [Date] TEXT,
            [Time] TEXT,
            [Time Zone] TEXT,
            [Modifiers Applied] TEXT,
            __row_key TEXT
        );
        CREATE UNIQUE INDEX IF NOT EXISTS idx_tx_row_key ON transactions(__row_key);
        CREATE TABLE IF NOT EXISTS inventory (
            [Product ID] TEXT,
            [Product Name] TEXT,
            SKU TEXT,
            Categories TEXT,
            Price REAL,
            [Tax - GST (10%)] TEXT,
            [Current Quantity Vie Market & Bar] REAL,
            [Default Unit Cost] REAL,
            Unit TEXT,
            source_date TEXT,
            [Stock on Hand] REAL
        );

        CREATE TABLE IF NOT EXISTS members (
            [Square Customer ID] TEXT,
            [First Name] TEXT,
            [Last Name] TEXT,
            [Email Address] TEXT,
            [Phone Number] TEXT,
            [Creation Date] TEXT,
            [Customer Note] TEXT,
            [Reference ID] TEXT
        );

        CREATE INDEX IF NOT EXISTS idx_txn_datetime ON transactions(Datetime);
        CREATE INDEX IF NOT EXISTS idx_txn_id ON transactions([Transaction ID]);
        CREATE INDEX IF NOT EXISTS idx_inv_sku ON inventory(SKU);
        CREATE INDEX IF NOT EXISTS idx_inv_categories ON inventory(Categories);
        CREATE INDEX IF NOT EXISTS idx_member_square ON members([Square Customer ID]);
        CREATE INDEX IF NOT EXISTS idx_member_ref ON members([Reference ID]);
    """)
    tmp_conn.commit()

    # --- 3) æ‹‰ Drive æ–‡ä»¶åˆ—è¡¨ï¼ˆå…¨é‡åˆ†é¡µï¼‰ï¼Œå¹¶åšâ€œåˆ—è¡¨å®Œæ•´æ€§â€ä¿æŠ¤ ---
    drive = get_drive()
    files = list_all_files_in_folder(drive, FOLDER_ID)
    log_info(f"ğŸ“¦ Drive files fetched: {len(files)}")
    # --- å†å²æ–‡ä»¶æ•°å®Œæ•´æ€§æ ¡éªŒ ---
    expected_count = load_expected_drive_file_count(main_db)
    if (not is_initial_build) and expected_count:
        current_count = len(files)
        # å®¹å¿ 10% æ³¢åŠ¨
        if current_count < int(expected_count * 0.9):
            log_warning(
                f"ğŸ›‘ Drive file count too small: {current_count} < 90% of expected {expected_count}. Abort ingest."
            )
            try:
                tmp_conn.close()
            except Exception:
                pass
            try:
                if os.path.exists(tmp_db):
                    os.remove(tmp_db)
            except Exception:
                pass
            return

    if not files:
        log_warning("âš ï¸ No files found in Drive folder.")
        try:
            tmp_conn.close()
        except Exception:
            pass
        try:
            if os.path.exists(tmp_db):
                os.remove(tmp_db)
        except Exception:
            pass
        return

    # --- 4) ç¨³å®šæ’åºï¼ˆç¡®å®šæ€§ï¼Œè·¨æœºå™¨ä¸€è‡´ï¼‰ ---
    def sort_key(f):
        name = f.get("title") or ""
        file_id = f.get("id") or ""
        start_date, _ = _extract_date_range_from_filename(name)

        sd_ts = pd.to_datetime(start_date, errors="coerce") if start_date else pd.NaT
        sd_val = int(sd_ts.value) if not pd.isna(sd_ts) else -1
        missing = 1 if sd_val == -1 else 0
        return (missing, -sd_val, name.strip().lower(), file_id)

    files = sorted(files, key=sort_key)

    # è¯Šæ–­ï¼šæ‰“å°æ—¥æœŸè·¨åº¦
    dates = []
    for f in files:
        name = f.get("title") or ""
        sd, _ = _extract_date_range_from_filename(name)
        if sd:
            dates.append(sd)
    if dates:
        log_info(f"ğŸ§­ File date span (by filename): {min(dates)}  â†’  {max(dates)}")

    seen = set()
    error_files = []
    attempted_supported = 0
    succeeded_supported = 0

    # --- 5) å¼€å§‹å¯¼å…¥åˆ°ä¸´æ—¶åº“ ---
    for f in files:
        name = f.get("title") or ""
        if not name:
            continue

        if name in seen:
            continue
        seen.add(name)

        local = os.path.join(tempfile.gettempdir(), name)

        try:
            is_csv = name.lower().endswith(".csv")
            is_xlsx = name.lower().endswith(".xlsx")

            # å…ˆç»Ÿè®¡â€œæ”¯æŒç±»å‹æ–‡ä»¶æ€»æ•°â€
            if is_csv or is_xlsx:
                attempted_supported += 1
            else:
                log_warning(f"âš ï¸ Skip unsupported file: {name}")
                continue

            # ä¸‹è½½ï¼ˆå¸¦é‡è¯•ï¼‰
            drive_get_content_file_with_retry(f, local, retries=3)

            # è¯»å–
            if is_csv:
                df = pd.read_csv(local)
            else:
                header_row = 1 if "catalogue" in name.lower() else 0
                df = pd.read_excel(local, header=header_row)

            df = _fix_header(df)

            # åˆ¤æ–­ç±»å‹ & å†™å…¥ä¸´æ—¶åº“
            if _is_transaction_file(name, df):
                df = preprocess_transactions(df)

                if "Datetime" in df.columns:
                    dt = pd.to_datetime(df["Datetime"], errors="coerce")
                    nat_rate = float(dt.isna().mean())
                    if nat_rate >= 0.8:
                        log_error(f"âŒ TX {name}: Datetime parse failed (NaT={nat_rate:.1%}), skipped")
                        error_files.append((name, f"Datetime NaT {nat_rate:.1%}"))
                        continue
                else:
                    log_error(f"âŒ TX {name}: missing Datetime column, skipped")
                    error_files.append((name, "Missing Datetime column"))
                    continue

                inserted = _write_df(
                    tmp_conn, df, "transactions",
                    key_candidates=["Transaction ID", "Item", "Price", "Modifiers Applied"],
                    prefer_real={"Net Sales", "Gross Sales", "Qty", "Discounts"},
                    is_initial_build=is_initial_build
                )

                succeeded_supported += 1
                if inserted == 0:
                    log_warning(f"âš ï¸ TX {name}: inserted=0 (likely deduped)")
                log_info(f"âœ… TX {name}: read={len(df)} inserted={inserted} NaT={nat_rate:.1%}")

            elif _is_inventory_file(name, df):
                df = preprocess_inventory(df, filename=name)
                inserted = _write_df(
                    tmp_conn, df, "inventory",
                    key_candidates=["SKU"], prefer_real=set(),
                    is_initial_build=is_initial_build
                )

                succeeded_supported += 1
                if inserted == 0:
                    log_warning(f"âš ï¸ INV {name}: inserted=0 (likely deduped)")
                log_info(f"âœ… INV {name}: read={len(df)} inserted={inserted}")

            elif _is_member_file(name, df):
                try:
                    df_members = preprocess_members(df)
                    inserted = _write_df(
                        tmp_conn, df_members, "members",
                        key_candidates=["Square Customer ID", "Reference ID"],
                        prefer_real=set(),
                        is_initial_build=is_initial_build
                    )

                    succeeded_supported += 1
                    log_info(f"ğŸ“¥ Members {name}: read={len(df_members)} inserted={inserted}")
                except Exception as e:
                    error_files.append((name, str(e)))
                    log_error(f"âŒ Failed to import members from {name}: {e}")

            else:
                log_warning(f"âš ï¸ Schema not recognized, skipped: {name}")
                # è¯†åˆ«ä¸äº†ä¸ç®—â€œæˆåŠŸå¯¼å…¥â€ï¼Œä½†ä¹Ÿä¸ç®—é”™è¯¯
                continue

        except Exception as e:
            msg = str(e)
            error_files.append((name, msg))
            log_error(f"âŒ Failed to import {name}: {msg}")

        finally:
            try:
                if os.path.exists(local):
                    os.remove(local)
            except Exception:
                pass

    # --- 6) å®Œæ•´æ€§æ£€æŸ¥ï¼šé¿å…åŠæ®‹ tmp è¦†ç›–æ­£å¼åº“ ---
    try:
        tx_rows = tmp_conn.execute("SELECT COUNT(*) FROM transactions").fetchone()[0]
    except Exception:
        tx_rows = 0

    # è§„åˆ™ Aï¼šæˆåŠŸç‡é˜ˆå€¼ï¼ˆé¿å…åªå¯¼å…¥å°‘é‡ supported æ–‡ä»¶ï¼‰
    # attempted_supported æ˜¯ .csv/.xlsx çš„æ•°é‡ï¼›æˆåŠŸç‡è¿‡ä½è¯´æ˜ä¸­é€”å¤±è´¥æˆ–åˆ—è¡¨ä¸å®Œæ•´
    success_rate = (succeeded_supported / attempted_supported) if attempted_supported else 0.0
    MIN_SUCCESS_RATE = 0.90  # å»ºè®®ä¼šå‰å…ˆ 0.90ï¼Œç¨³å®šåå¯ä»¥å‡åˆ° 0.95

    # è§„åˆ™ Bï¼šæœ€æ—©æ—¥æœŸå¯¹é½æ ¡éªŒï¼ˆå…³é”®ï¼‰
    # - min_file_dateï¼šä»æ–‡ä»¶åè§£æåˆ°çš„æœ€æ—©æ—¥æœŸï¼ˆåªçœ‹èƒ½è§£æå‡ºæ—¥æœŸçš„æ–‡ä»¶ï¼‰
    # - min_tx_dateï¼štmp åº“ transactions çš„æœ€æ—© Datetime æ—¥æœŸ
    min_file_date = None
    try:
        parsed_dates = []
        for f in files:
            name = f.get("title") or ""
            sd, _ = _extract_date_range_from_filename(name)
            if sd:
                d = pd.to_datetime(sd, errors="coerce")
                if not pd.isna(d):
                    parsed_dates.append(d.normalize())
        if parsed_dates:
            min_file_date = min(parsed_dates)
    except Exception:
        min_file_date = None

    min_tx_date = None
    try:
        # Datetime å­˜çš„æ˜¯å­—ç¬¦ä¸²ï¼Œè¿™é‡Œç”¨ sqlite çš„ date() æŠ½å–æ—¥æœŸ
        row = tmp_conn.execute("SELECT MIN(date(Datetime)) FROM transactions").fetchone()
        if row and row[0]:
            min_tx_date = pd.to_datetime(row[0], errors="coerce").normalize()
    except Exception:
        min_tx_date = None

    log_info(
        f"ğŸ“Š TMP DB check: tx_rows={tx_rows}, attempted_files={attempted_supported}, "
        f"succeeded_files={succeeded_supported}, success_rate={success_rate:.1%}, "
        f"min_file_date={min_file_date.date() if min_file_date is not None else None}, "
        f"min_tx_date={min_tx_date.date() if min_tx_date is not None else None}"
    )

    # âœ… åˆ¤å®šï¼šå¦‚æœèƒ½è§£æå‡º min_file_dateï¼Œä½† tmp çš„æœ€æ—©äº¤æ˜“æ—¥æœŸæ˜æ˜¾æ™šäºå®ƒï¼ˆå®¹å·® 1 å¤©ï¼‰ï¼Œè¯´æ˜æ¼äº†æ—©æœŸå¤§æ®µæ–‡ä»¶
    DATE_TOLERANCE_DAYS = 1
    date_ok = True
    if min_file_date is not None and min_tx_date is not None:
        date_ok = (min_tx_date <= (min_file_date + pd.Timedelta(days=DATE_TOLERANCE_DAYS)))

    # âœ… æœ€ç»ˆ gateï¼šæˆåŠŸç‡ + æ—¥æœŸå¯¹é½
    if (success_rate < MIN_SUCCESS_RATE) or (not date_ok):
        reason = []
        if success_rate < MIN_SUCCESS_RATE:
            reason.append(f"success_rate<{MIN_SUCCESS_RATE:.0%}")
        if not date_ok:
            reason.append(f"min_tx_date>{DATE_TOLERANCE_DAYS}d after min_file_date")
        log_warning(
            f"ğŸ›‘ TMP DB aborted summary: "
            f"attempted_supported={attempted_supported}, "
            f"succeeded_supported={succeeded_supported}, "
            f"success_rate={success_rate:.1%}, "
            f"min_file_date={min_file_date.date() if min_file_date is not None else None}, "
            f"min_tx_date={min_tx_date.date() if min_tx_date is not None else None}"
        )

        try:
            tmp_conn.close()
        except Exception:
            pass
        try:
            if os.path.exists(tmp_db):
                os.remove(tmp_db)
        except Exception:
            pass
        return

    # --- 7) åŸå­æ›¿æ¢ï¼štmp -> mainï¼ˆå¹¶å¤‡ä»½ï¼‰ ---
    try:
        tmp_conn.close()
    except Exception:
        pass

    try:
        if os.path.exists(bak_db):
            os.remove(bak_db)
    except Exception:
        pass

    try:
        if os.path.exists(main_db):
            shutil.copy2(main_db, bak_db)  # å¤‡ä»½æ—§åº“
        os.replace(tmp_db, main_db)  # åŸå­æ›¿æ¢ï¼ˆåŒç›˜ï¼‰
        log_info("âœ… TMP DB committed: replaced main DB successfully.")

        # --- è®°å½•â€œå†å²æ­£å¸¸æ–‡ä»¶æ•°â€ä½œä¸ºåŸºå‡†ï¼ˆå†™ db æ—è¾¹ jsonï¼Œä¸æ‰“å¼€ä¸»åº“ï¼‰ ---
        ok = save_expected_drive_file_count(main_db, len(files))
        if ok:
            log_info(f"ğŸ“Œ Saved expected_drive_file_count={len(files)} (json next to db)")


    except Exception as e:
        log_error(f"âŒ Failed to replace main DB: {e}")
        # æ›¿æ¢å¤±è´¥ï¼šä¿ç•™æ—§åº“ï¼Œæ¸…ç† tmp
        try:
            if os.path.exists(tmp_db):
                os.remove(tmp_db)
        except Exception:
            pass

    # --- 8) æ‰“å°å¤±è´¥æ–‡ä»¶æ±‡æ€»ï¼ˆä¸å½±å“æˆåŠŸæäº¤ï¼‰ ---
    if error_files:
        log_warning("âš ï¸ Some Drive files were skipped when building database:")
        for fname, _ in error_files:
            log_info(f"â€¢ {fname}")


def init_db_from_drive_once():
    """
    è‡ªåŠ¨åˆå§‹åŒ–æ•°æ®åº“ï¼ˆä»…åœ¨åº“ä¸ºç©ºæ—¶ï¼‰
    - ä½¿ç”¨æ–‡ä»¶é”ï¼Œé¿å…å¹¶å‘
    - ä¸å†ä½¿ç”¨ ingest_meta è¡¨
    """
    with ingest_file_lock() as locked:
        if not locked:
            log_warning("â³ ingest already running (file lock), skip init_db_from_drive_once()")
            try:
                st.info("Database is initializing in another process. Please wait a moment.")
            except Exception:
                pass
            return False

        try:
            conn = get_db()
            cur = conn.cursor()

            try:
                tx_count = cur.execute("SELECT COUNT(*) FROM transactions").fetchone()[0]
                inv_count = cur.execute("SELECT COUNT(*) FROM inventory").fetchone()[0]
            except Exception:
                tx_count, inv_count = 0, 0

            conn.close()

            # åªæœ‰åœ¨â€œçœŸæ­£ç©ºåº“â€æ—¶æ‰ ingest
            if tx_count == 0 and inv_count == 0:
                log_info("ğŸš€ Empty DB detected, ingesting from Drive...")
                _ingest_from_drive_all_impl()

            return True

        except Exception as e:
            log_warning(f"âš ï¸ Auto-ingest from Drive failed: {e}")
            return False




# --------------- æ‰‹åŠ¨å¯¼å…¥ï¼ˆSidebar ä¸Šä¼ ï¼‰ ---------------
def ingest_csv(uploaded_file, source_file=None):
    data = uploaded_file.getvalue() if hasattr(uploaded_file, "getvalue") else uploaded_file.read()
    filename = uploaded_file.name if hasattr(uploaded_file, "name") else "uploaded.csv"
    conn = get_db()
    ensure_indexes()
    log_info(f"ğŸ“‚ Importing {filename}")

    try:
        df = pd.read_csv(BytesIO(data))
        df = _fix_header(df)

        if _is_transaction_file(filename, df):
            df = preprocess_transactions(df)
            inserted = _write_df(conn, df, "transactions",
                                 key_candidates=["Transaction ID"],
                                 prefer_real={"Net Sales", "Gross Sales", "Qty", "Discounts"})

        elif _is_inventory_file(filename, df):
            df = preprocess_inventory(df, filename=filename)
            inserted = _write_df(conn, df, "inventory",
                                 key_candidates=["SKU"], prefer_real=set())

        elif _is_member_file(filename, df):
            df = preprocess_members(df)
            inserted = _write_df(conn, df, "members",
                                 key_candidates=["Square Customer ID", "Reference ID"], prefer_real=set())
        else:
            log_warning(f"âš ï¸ Skipped {filename}, schema not recognized")
            return False
        # === NEW: record ingested file ===
        conn.execute(
            """
            INSERT OR IGNORE INTO ingestion_log (source_file)
            VALUES (?)
            """,
            (filename,)
        )

        # ä¸Šä¼ åˆ° Google Driveï¼ˆç”¨åŒä¸€ä»½ dataï¼Œæ°¸ä¸ä¸ºç©ºï¼‰
        tmp_path = os.path.join(tempfile.gettempdir(), filename)
        with open(tmp_path, "wb") as f_local:
            f_local.write(data)

        uploaded_drive_files = st.session_state.get("uploaded_drive_files", set())
        if filename not in uploaded_drive_files:
            upload_file_to_drive(tmp_path, filename)
            uploaded_drive_files.add(filename)
        st.session_state["uploaded_drive_files"] = uploaded_drive_files

        ensure_indexes()
        return True

    except Exception as e:
        log_error(f"âŒ Error importing {filename}: {str(e)}")
        return False
    finally:
        try:
            conn.close()
        except Exception:
            pass
        try:
            if 'tmp_path' in locals():
                os.remove(tmp_path)
        except Exception:
            pass


def ingest_excel(uploaded_file):
    conn = get_db()
    ensure_indexes()

    filename = uploaded_file.name if hasattr(uploaded_file, "name") else "uploaded.xlsx"
    log_info(f"ğŸ“‚ Importing {filename}")

    try:
        data = uploaded_file.getvalue() if hasattr(uploaded_file, "getvalue") else uploaded_file.read()
        xls = pd.ExcelFile(BytesIO(data))

        total_rows_imported = 0

        is_catalogue = ("catalogue" in filename.lower())
        # åªå¤„ç† Items/é¦–ä¸ª sheetï¼ˆåº“å­˜ï¼‰
        if is_catalogue:
            target_sheets = []
            if "Items" in xls.sheet_names:
                target_sheets = ["Items"]
            else:
                # å›é€€ï¼šæ‰¾ç¬¬ä¸€ä¸ª sheet
                target_sheets = [xls.sheet_names[0]]

            inv_frames = []
            for sheet in target_sheets:
                df = pd.read_excel(xls, sheet_name=sheet, header=1)
                df = _fix_header(df)
                if ("SKU" in df.columns) or ("Stock on Hand" in df.columns) or ("Categories" in df.columns):
                    df = preprocess_inventory(df, filename=filename)
                    inv_frames.append(df)

            if inv_frames:
                inv_all = pd.concat(inv_frames, ignore_index=True)
                inserted = _write_df(conn, inv_all, "inventory",
                                     key_candidates=["SKU"], prefer_real=set())
                total_rows_imported += inserted

        else:
            # é catalogue çš„ Excelï¼šä¿ç•™åŸé€»è¾‘ï¼ˆé€ sheet å¯¼å…¥ï¼‰
            for sheet in xls.sheet_names:
                header_row = 0
                df = pd.read_excel(xls, sheet_name=sheet, header=header_row)
                df = _fix_header(df)

                if _is_transaction_file(filename, df):
                    df = preprocess_transactions(df)
                    inserted = _write_df(conn, df, "transactions",
                                         key_candidates=["Transaction ID"],
                                         prefer_real={"Net Sales", "Gross Sales", "Qty", "Discounts"})
                    total_rows_imported += inserted


                elif _is_inventory_file(filename, df):
                    df = preprocess_inventory(df, filename=filename)
                    inserted = _write_df(conn, df, "inventory",
                                         key_candidates=["SKU"], prefer_real=set())
                    total_rows_imported += inserted

                elif _is_member_file(filename, df):
                    df = preprocess_members(df)
                    inserted = _write_df(conn, df, "members",
                                         key_candidates=["Square Customer ID", "Reference ID"], prefer_real=set())
                    total_rows_imported += inserted

        log_info(f"âœ… {filename} imported - {total_rows_imported} total rows")

        # === NEW: record ingested file ===
        conn.execute(
            """
            INSERT OR IGNORE INTO ingestion_log (source_file)
            VALUES (?)
            """,
            (filename,)
        )


        # ä¸Šä¼ åˆ° Driveï¼ˆä¿æŒåŸé€»è¾‘ï¼‰
        tmp_path = os.path.join(tempfile.gettempdir(), filename)
        with open(tmp_path, "wb") as f_local:
            f_local.write(data)
        # === é˜²æ­¢é‡å¤ä¸Šä¼ åˆ° Google Drive ===
        uploaded_drive_files = st.session_state.get("uploaded_drive_files", set())
        if filename not in uploaded_drive_files:
            upload_file_to_drive(tmp_path, filename)
            uploaded_drive_files.add(filename)
        st.session_state["uploaded_drive_files"] = uploaded_drive_files

        ensure_indexes()
        return True

    except Exception as e:
        log_error(f"âŒ Error importing {filename}: {str(e)}")
        return False
    finally:
        try:
            conn.close()
        except Exception:
            pass
        try:
            if 'tmp_path' in locals():
                os.remove(tmp_path)
        except Exception:
            pass

def ingest_new_files_from_drive_only():
    """
    åª ingest Google Drive ä¸­ã€Œè¿˜æ²¡è¿›è¿‡æ•°æ®åº“ã€çš„æ–°æ–‡ä»¶
    ä¸ rebuildï¼Œä¸æ¸…åº“
    """
    from services.db import get_db

    conn = get_db()
    init_database()

    # 1. å–æ•°æ®åº“é‡Œå·²æœ‰çš„æ–‡ä»¶åï¼ˆç”¨äºå»é‡ï¼‰
    existing_files = set()

    try:
        rows = conn.execute("""
            SELECT DISTINCT source_file
            FROM ingestion_log
        """).fetchall()
        existing_files = {r[0] for r in rows if r[0]}
    except Exception:
        # ç¬¬ä¸€æ¬¡è·‘å¯èƒ½è¿˜æ²¡ ingestion_log è¡¨
        pass

    drive = get_drive()
    files = list_all_files_in_folder(drive, FOLDER_ID)

    new_files = []

    for f in files:
        name = f.get("title", "").lower()
        if not (name.endswith(".csv") or name.endswith(".xlsx")):
            continue

        if f["title"] in existing_files:
            continue  # å·² ingestï¼Œè·³è¿‡

        new_files.append(f)

    if not new_files:
        return 0  # æ²¡æœ‰æ–°æ–‡ä»¶

    # 2. ingest æ–°æ–‡ä»¶
    import tempfile, os, pandas as pd

    for f in new_files:
        file_id = f["id"]
        filename = f["title"]
        local = os.path.join(tempfile.gettempdir(), filename)

        drive_file = drive.CreateFile({'id': file_id})
        drive_get_content_file_with_retry(drive_file, local)

        # === è¯»å–æ–‡ä»¶ ===
        if filename.lower().endswith(".csv"):
            df = pd.read_csv(local)
        else:
            header_row = 1 if "catalogue" in filename.lower() else 0
            df = pd.read_excel(local, header=header_row)

        df = _fix_header(df)

        # === åˆ¤æ–­ç±»å‹å¹¶å†™å…¥ä¸»åº“ï¼ˆå’Œå…¨é‡ ingest åŒä¸€é€»è¾‘ï¼‰ ===
        if _is_transaction_file(filename, df):
            df = preprocess_transactions(df)
            _write_df(
                conn, df, "transactions",
                key_candidates=["Transaction ID"],
                prefer_real={"Net Sales", "Gross Sales", "Qty", "Discounts"},
                is_initial_build=False
            )

        elif _is_inventory_file(filename, df):
            df = preprocess_inventory(df, filename=filename)
            _write_df(
                conn, df, "inventory",
                key_candidates=["SKU"],
                prefer_real=set(),
                is_initial_build=False
            )

        elif _is_member_file(filename, df):
            df = preprocess_members(df)
            _write_df(
                conn, df, "members",
                key_candidates=["Square Customer ID", "Reference ID"],
                prefer_real=set(),
                is_initial_build=False
            )
        else:
            log_warning(f"âš ï¸ Drive file skipped (schema not recognized): {filename}")
            continue

        # === è®°å½• ingestion_logï¼ˆå…³é”®ï¼‰ ===
        conn.execute(
            """
            INSERT OR IGNORE INTO ingestion_log (source_file)
            VALUES (?)
            """,
            (filename,)
        )

    try:
        conn.commit()
        return len(new_files)
    finally:
        try:
            conn.close()
        except Exception:
            pass


__all__ = [
    "ingest_csv",
    "ingest_excel",
    "ingest_from_drive_all",
    "get_drive",
    "upload_file_to_drive",
    "download_file_from_drive",
    "init_db_from_drive_once",
]