import streamlit as st
from datetime import datetime, timedelta, date
import pandas as pd

from services.logger import init_logging, LOG_FILE, log_info, log_warning, log_error

st.set_page_config(
    page_title="Vie Manly Analytics",
    layout="wide",
    initial_sidebar_state="auto"
)
init_logging()


import warnings
warnings.filterwarnings("ignore", category=FutureWarning)
from services.db import get_db_path
import os
import pandas as pd
from services.analytics import load_all, rebuild_high_level_summary, rebuild_inventory_summary
from services.db import get_db, init_database
from services.ingestion import ingest_excel, ingest_csv, init_db_from_drive_once
from charts.high_level import show_high_level
from charts.sales_report import show_sales_report
from charts.inventory import show_inventory
from charts.product_mix_only import show_product_mix_only
from charts.customer_segmentation import show_customer_segmentation
from init_db import init_db
import subprocess
import sys
from services.ingestion import ingest_from_drive_all
import platform
import numpy as np
from datetime import datetime, timedelta

import psutil

init_database()

def check_memory():
    mem = psutil.virtual_memory()
    used_gb = mem.used / (1024 ** 3)
    total_gb = mem.total / (1024 ** 3)
    usage_ratio = used_gb / total_gb

    if usage_ratio > 0.85:
        st.warning(f"âš ï¸ Memory usage high ({usage_ratio*100:.1f}%). Please refresh occasionally.")



# å…³é—­æ–‡ä»¶ç›‘æ§ï¼Œé¿å… Streamlit Cloud æŠ¥ inotify é”™è¯¯
os.environ["WATCHDOG_DISABLE_FILE_WATCH"] = "true"

# âœ… ç¡®ä¿ SQLite æ–‡ä»¶å’Œè¡¨ç»“æ„å­˜åœ¨
init_db()  # å¿…é¡»å…ˆåˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„

if "drive_initialized" not in st.session_state:
    ok = init_db_from_drive_once()
    if ok:
        rebuild_high_level_summary()
        rebuild_inventory_summary()
        st.session_state.drive_initialized = True
    else:
        # ingest è¢«é” / Drive æœª readyï¼Œä¸è¦æ ‡è®°å·²åˆå§‹åŒ–
        st.warning("â³ Database is initializing. Please wait...")
        st.stop()



st.markdown("<h1 style='font-size:26px; font-weight:700;'>ğŸ“Š Vie Manly Dashboard</h1>", unsafe_allow_html=True)

@st.cache_data(show_spinner="loading...")
def load_db_cached(db_mtime):
    db = get_db()
    return load_all(db=db)

def reload_db_cache():
    st.session_state.pop("db_cache", None)
    db_path = get_db_path()
    db_mtime = os.path.getmtime(db_path)
    st.session_state.db_cache = load_db_cached(db_mtime)

BAD_DATES = {
    date(2025, 8, 18),
    date(2025, 8, 19),
    date(2025, 8, 20),
}
def check_missing_data(tx, inv):
    """
    åˆ†å¼€æ£€æŸ¥äº¤æ˜“å’Œåº“å­˜çš„ç¼ºå¤±æ—¥æœŸï¼š

    - äº¤æ˜“ï¼ˆtransactionsï¼‰ï¼š
        * ä»å›ºå®šçš„èµ·å§‹æ—¥æœŸ tx_start_date å¼€å§‹ï¼ˆä½ å¯ä»¥æ ¹æ®éœ€è¦æ”¹ï¼‰
        * åˆ°ä»Šå¤©ä¸ºæ­¢ï¼Œæ¯ä¸€å¤©å¦‚æœåœ¨æ•°æ®åº“é‡Œå®Œå…¨æ²¡æœ‰äº¤æ˜“è®°å½•ï¼Œå°±æ ‡è®°ä¸ºç¼ºå¤±

    - åº“å­˜ï¼ˆinventoryï¼‰ï¼š
        * ä»å›ºå®šçš„èµ·å§‹æ—¥æœŸ inv_start_date å¼€å§‹ï¼ˆä½ æ˜ç¡®è¯´è¦ä» 2025-11-01ï¼‰
        * åˆ°ä»Šå¤©ä¸ºæ­¢ï¼Œæ¯ä¸€å¤©å¦‚æœåœ¨æ•°æ®åº“é‡Œæ²¡æœ‰ä»»ä½• inventory è®°å½•ï¼Œå°±æ ‡è®°ä¸ºç¼ºå¤±
    """
    missing_info = {
        "transaction_dates": [],
        "inventory_dates": [],
    }

    today = datetime.now().date()

    # ===== 1. äº¤æ˜“ç¼ºå¤±æ£€æŸ¥ =====
    # å¦‚æœä½ ä»¥åæƒ³æ”¹æˆä» 2024-01-01 å¼€å§‹æ£€æµ‹ï¼Œå¯ä»¥æŠŠä¸‹é¢è¿™è¡Œæ”¹æˆ date(2024, 1, 1)
    tx_start_date = date(2024, 1, 1)

    if tx is not None and not tx.empty and "Datetime" in tx.columns:
        # æŠŠ Datetime åˆ—å®‰å…¨åœ°è½¬æˆæ—¥æœŸ
        tx_dates_series = pd.to_datetime(tx["Datetime"], errors="coerce").dt.date
        tx_dates = set(d for d in tx_dates_series.dropna())

        # åªåœ¨ tx_start_date ~ today è¿™ä¸ªåŒºé—´å†…æ£€æŸ¥
        if tx_start_date <= today:
            all_days = [
                tx_start_date + timedelta(days=i)
                for i in range((today - tx_start_date).days + 1)
            ]
            missing_tx = [
                d for d in all_days
                if d not in tx_dates and d not in BAD_DATES
            ]

            missing_info["transaction_dates"] = missing_tx

    # ===== 2. åº“å­˜ç¼ºå¤±æ£€æŸ¥ =====
    # æŒ‰ä½ çš„è¦æ±‚ï¼šinventory å›ºå®šä» 2025-11-01 å¾€åæ£€æŸ¥
    inv_start_date = date(2025, 11, 1)

    if inv is not None and not inv.empty and "source_date" in inv.columns:
        inv_dates_series = pd.to_datetime(inv["source_date"], errors="coerce").dt.date
        inv_dates = set(d for d in inv_dates_series.dropna())

        if inv_start_date <= today:
            all_days = [
                inv_start_date + timedelta(days=i)
                for i in range((today - inv_start_date).days + 1)
            ]
            missing_inv = [d for d in all_days if d not in inv_dates]
            missing_info["inventory_dates"] = missing_inv

    return missing_info

if "db_auto_reloaded" not in st.session_state:
    reload_db_cache()
    st.session_state.db_auto_reloaded = True

tx, mem, inv = st.session_state.db_cache


# === Sidebar ===
st.sidebar.header("âš™ï¸ Settings")

# === æ•°æ®ç¼ºå¤±é¢„è­¦ ===
missing_data = check_missing_data(tx, inv)

if missing_data['transaction_dates'] or missing_data['inventory_dates']:
    st.sidebar.markdown("---")
    st.sidebar.markdown("### âš ï¸ Data missing warning")

    if missing_data['transaction_dates']:
        st.sidebar.error("**Missing transaction date:**")
        # æ˜¾ç¤ºæœ€è¿‘7å¤©çš„ç¼ºå¤±æ—¥æœŸï¼Œå…¶ä»–çš„æŠ˜å æ˜¾ç¤º
        recent_missing = sorted(missing_data['transaction_dates'])[-7:]
        for date in recent_missing:
            st.sidebar.write(f"ğŸ“… {date.strftime('%Y-%m-%d')}")

        if len(missing_data['transaction_dates']) > 7:
            with st.sidebar.expander(f"check all {len(missing_data['transaction_dates'])} missing dates"):
                for date in sorted(missing_data['transaction_dates']):
                    st.write(f"ğŸ“… {date.strftime('%Y-%m-%d')}")

    if missing_data['inventory_dates']:
        st.sidebar.warning("**Missing inventory date:**")
        # æ˜¾ç¤ºæœ€è¿‘7å¤©çš„ç¼ºå¤±æ—¥æœŸï¼Œå…¶ä»–çš„æŠ˜å æ˜¾ç¤º
        recent_missing = sorted(missing_data['inventory_dates'])[-7:]
        for date in recent_missing:
            st.sidebar.write(f"ğŸ“¦ {date.strftime('%Y-%m-%d')}")

        if len(missing_data['inventory_dates']) > 7:
            with st.sidebar.expander(f"check all {len(missing_data['inventory_dates'])} missing dates"):
                for date in sorted(missing_data['inventory_dates']):
                    st.write(f"ğŸ“¦ {date.strftime('%Y-%m-%d')}")

# æ–‡ä»¶ä¸Šä¼  - æ·»åŠ ä¸Šä¼ çŠ¶æ€è·Ÿè¸ª
if "uploaded_file_names" not in st.session_state:
    st.session_state.uploaded_file_names = set()

uploaded_files = st.sidebar.file_uploader(
    "Upload files",
    type=["csv", "xlsx"],
    accept_multiple_files=True
)

# âœ… æ”¹é€ ï¼šä¸Šä¼ åªâ€œæš‚å­˜â€ï¼Œç‚¹å‡»æŒ‰é’®æ‰ ingestï¼ˆåªå¤„ç†æ–°æ–‡ä»¶ï¼Œä¸é‡å»º DBï¼‰
if "pending_uploads" not in st.session_state:
    st.session_state.pending_uploads = {}  # {filename: UploadedFile}

# æ”¶é›†æœ¬æ¬¡é€‰æ‹©çš„â€œæ–°æ–‡ä»¶â€ï¼ˆä¸ç«‹åˆ» ingestï¼‰
if uploaded_files:
    for f in uploaded_files:
        # è¿™ä¸ª session é‡Œå·²ç» ingest è¿‡çš„ï¼Œç›´æ¥è·³è¿‡
        if f.name in st.session_state.uploaded_file_names:
            continue
        # æš‚å­˜ç­‰å¾…æŒ‰é’®è§¦å‘ ingest
        st.session_state.pending_uploads[f.name] = f

# ç»™ç”¨æˆ·æç¤º
if st.session_state.pending_uploads:
    st.sidebar.info(
        f"ğŸ“¥ {len(st.session_state.pending_uploads)} new file(s) ready. "
        f"Click 'ğŸ”„ Refresh New Files' to ingest."
    )

if st.sidebar.button("ğŸ”„ Refresh New Files"):
    from services.ingestion import ingest_csv, ingest_excel, ingest_file_lock
    from services.analytics import update_high_level_summary_by_db_diff, rebuild_inventory_summary

    ingested_any = False

    with ingest_file_lock() as locked:
        if not locked:
            st.warning("â³ Another ingestion is running. Please try again.")
            st.stop()

        # === 1ï¸âƒ£ Ingest æ–°æ–‡ä»¶ ===
        pending = list(st.session_state.get("pending_uploads", {}).items())
        for filename, uploaded_file in pending:
            try:
                if filename.lower().endswith(".csv"):
                    ingest_csv(uploaded_file, filename)
                elif filename.lower().endswith(".xlsx"):
                    ingest_excel(uploaded_file, filename)
                st.session_state.uploaded_file_names.add(filename)
                ingested_any = True
            except Exception as e:
                st.error(f"âŒ Failed to ingest {filename}: {e}")

        st.session_state.pending_uploads.clear()

    # === 2ï¸âƒ£ ğŸ”¥ å…³é”®ï¼šå…ˆé‡ç®— Summaryï¼Œå†åˆ·æ–°ç¼“å­˜ ===
    # å¿…é¡»åœ¨ reload_db_cache ä¹‹å‰æ‰§è¡Œï¼Œç¡®ä¿æ•°æ®åº“é‡Œçš„ Summary è¡¨æ˜¯æœ€æ–°çš„
    update_high_level_summary_by_db_diff()
    rebuild_inventory_summary()

    # === 3ï¸âƒ£ ğŸ”¥ å…³é”®ï¼šæ¸…é™¤ Streamlit çš„æ‰€æœ‰æ•°æ®ç¼“å­˜ ===
    # è¿™ä¼šå¼ºåˆ¶ load_summary() ç­‰å‡½æ•°é‡æ–°ä»æ•°æ®åº“è¯»å–æœ€æ–°çš„ Summary è¡¨
    st.cache_data.clear()

    # === 4ï¸âƒ£ åˆ·æ–°åŸå§‹æ•°æ®çš„ cache (tx, mem, inv) ===
    reload_db_cache()

    if ingested_any:
        st.success("âœ… New files ingested and data refreshed")
    else:
        st.info("â„¹ï¸ Data reloaded")

    st.rerun()

# ===============================
# ğŸ› ï¸ Database maintenance
# ===============================
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ› ï¸ Database")

if st.sidebar.button("Clear & Rebuild Database"):
    # 1ï¸âƒ£ ç«‹å³æ¸…é™¤ Streamlit æ‰€æœ‰å…¨å±€å‡½æ•°ç¼“å­˜ (æ ¸å¿ƒæ­¥éª¤)
    # è¿™ç¡®ä¿äº† charts é‡Œçš„ load_summary ç­‰å‡½æ•°ä¸‹æ¬¡è¿è¡Œæ—¶å¿…è¯»æ•°æ®åº“ï¼Œè€Œä¸æ˜¯è¯»å†…å­˜
    st.cache_data.clear()

    # 2ï¸âƒ£ æ¸…ç† Session ä¸­çš„æ—§å¼•ç”¨
    if "db_cache" in st.session_state:
        del st.session_state["db_cache"]

    # 3ï¸âƒ£ æ‰§è¡Œä» Drive çš„å…¨é‡é‡æ–°åŒæ­¥
    # æ³¨æ„ï¼šç¡®ä¿ä½ çš„ ingest_from_drive_all å†…éƒ¨ä¼šå…ˆ DROP/TRUNCATE æ‰åŸå§‹è¡¨
    ok = ingest_from_drive_all()
    if not ok:
        st.sidebar.warning("â³ Database is busy. Please try again.")
        st.stop()

    # 4ï¸âƒ£ å¼ºåˆ¶åˆå§‹åŒ–è¡¨ç»“æ„
    init_database()

    # 5ï¸âƒ£ ç‰©ç†æ¸…ç©º Summary è¡¨ï¼Œç¡®ä¿æ²¡æœ‰æ®‹ç•™
    with get_db() as conn:
        conn.execute("DELETE FROM high_level_daily")
        conn.execute("DELETE FROM inventory_summary")
        conn.commit()

    # 6ï¸âƒ£ ç­‰å¾…æ•°æ®å†™å…¥ç‰©ç†ç£ç›˜çš„ç¼“å†²ï¼ˆé’ˆå¯¹æŸäº›ç¯å¢ƒä¸‹çš„ SQLite å»¶è¿Ÿï¼‰
    import time

    time.sleep(1)

    # 7ï¸âƒ£ æ‰§è¡Œé‡ç®—
    rebuild_high_level_summary()
    rebuild_inventory_summary()

    # 8ï¸âƒ£ é‡æ–°åŠ è½½ç¼“å­˜åˆ° session_state
    reload_db_cache()

    st.sidebar.success("âœ… Database fully rebuilt and cache cleared.")
    st.rerun()

# --- 2) Refresh (cache only) ---
if st.sidebar.button("ğŸ”„ Refresh data"):
    reload_db_cache()
    st.sidebar.success("Reloading dataâ€¦")
    st.rerun()


# --- 3) Debug Snapshot ---
if st.sidebar.button("Debug Snapshot"):
    try:
        conn = get_db()

        row = conn.execute("PRAGMA database_list").fetchone()
        db_path = row[2] if row and len(row) >= 3 else None

        log_info("ğŸ§ª DEBUG SNAPSHOT")
        log_info(f"ğŸ—„ï¸ DB path: {db_path}")

        tx_stats = conn.execute("""
            SELECT 
                MIN(date(Datetime)),
                MAX(date(Datetime)),
                COUNT(*),
                COUNT(DISTINCT date(Datetime))
            FROM transactions
        """).fetchone()

        log_info(
            f"ğŸ“Š transactions: min_date={tx_stats[0]}, "
            f"max_date={tx_stats[1]}, rows={tx_stats[2]}, "
            f"distinct_days={tx_stats[3]}"
        )

        inv_stats = conn.execute("""
            SELECT 
                MIN(source_date),
                MAX(source_date),
                COUNT(*),
                COUNT(DISTINCT source_date)
            FROM inventory
        """).fetchone()

        log_info(
            f"ğŸ“¦ inventory: min_date={inv_stats[0]}, "
            f"max_date={inv_stats[1]}, rows={inv_stats[2]}, "
            f"distinct_days={inv_stats[3]}"
        )

        st.sidebar.success("Debug snapshot written to log.")

    except Exception as e:
        log_error(f"âŒ DEBUG SNAPSHOT failed: {e}")
        st.sidebar.error("Debug snapshot failed. Check logs.")


with st.sidebar.expander("ğŸªµ Logs"):
    st.caption(f"Log file: {LOG_FILE}")
    try:
        log_text = LOG_FILE.read_text(encoding="utf-8")
    except Exception:
        log_text = ""
    tail = "\n".join(log_text.splitlines()[-60:])
    st.text_area("Latest log lines", tail, height=220)
    st.download_button("Download app.log", log_text, file_name="app.log", mime="text/plain")


# === å•ä½é€‰æ‹© ===
st.sidebar.subheader("ğŸ“ Units")

if inv is not None and not inv.empty and "Unit" in inv.columns:
    units_available = sorted(inv["Unit"].dropna().unique().tolist())
else:
    units_available = ["Gram 1.000", "Kilogram 1.000", "Milligram 1.000"]

conn = get_db()
try:
    rows = conn.execute("SELECT name FROM units").fetchall()
    db_units = [r[0] for r in rows]  # ä¿®å¤è¿™é‡Œçš„ç´¢å¼•é”™è¯¯
except Exception:
    db_units = []

all_units = sorted(list(set(units_available + db_units)))
unit = st.sidebar.selectbox("Choose unit", all_units)

new_unit = st.sidebar.text_input("Add new unit")
if st.sidebar.button("â• Add Unit"):
    if new_unit and new_unit not in all_units:
        conn.execute("CREATE TABLE IF NOT EXISTS units (name TEXT UNIQUE)")
        conn.execute("INSERT OR IGNORE INTO units (name) VALUES (?)", (new_unit,))
        conn.commit()
        st.sidebar.success(f"âœ… Added new unit: {new_unit}")
        st.rerun()

# === Section é€‰æ‹© ===
section = st.sidebar.radio("ğŸ“‚ Sections", [
    "High Level report",
    "Sales report by category",
    "Inventory",
    "product mix",
    "Customers insights"
])

# === ä¸»ä½“å±•ç¤º ===
if section == "High Level report":
    show_high_level(tx, mem, inv)
elif section == "Sales report by category":
    show_sales_report(tx, inv)
elif section == "Inventory":
    show_inventory(tx, inv)
elif section == "product mix":
    show_product_mix_only(tx)
elif section == "Customers insights":
    show_customer_segmentation(tx, mem)